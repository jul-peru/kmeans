{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81aa87d9-c8c8-4bef-bbb1-8a36df8abf29",
   "metadata": {
    "id": "81aa87d9-c8c8-4bef-bbb1-8a36df8abf29"
   },
   "source": [
    "### STRUCTURE FOR THE CLUSTERING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b769ed-4eb4-43f1-9ffc-e7c90f246265",
   "metadata": {
    "id": "89b769ed-4eb4-43f1-9ffc-e7c90f246265"
   },
   "source": [
    "1. EDA: data exploration and visualisation. To see what we have and what we can do about it. Explore data types, outliers (boxplot or use log), missings. Visualize them.\n",
    "2. Feature Engineering. We have to make features to be acceptable for model. Normalization between [0; 1]\n",
    "3. Model test, evalusation\n",
    "4. Model run, prediction\n",
    "5. Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56293d50-1954-4b1a-8d1e-f7e046c848dd",
   "metadata": {
    "id": "56293d50-1954-4b1a-8d1e-f7e046c848dd"
   },
   "source": [
    "When stuck on what to do next always go back to the description here.<br>\n",
    "\n",
    "Preparing variables for K-means clustering involves several important steps to ensure that the clustering process is effective and meaningful. Here are the key steps to prepare your variables for K-means:\n",
    "\n",
    "### Preparing Data\n",
    "#### 1. Data Collection and Cleaning:\n",
    "\n",
    "Gather your dataset, ensuring that it is complete and free of errors.<br>\n",
    "Handle any missing values in your dataset using techniques like imputation or removal of incomplete records.<br>\n",
    "\n",
    "#### 2. Feature Selection:\n",
    "\n",
    "Carefully select the features (variables) that are relevant to your clustering task. Including irrelevant features can negatively impact the results.<br>\n",
    "\n",
    "#### 3. Normalization/Standardization:\n",
    "\n",
    "Normalize or standardize your data to ensure that all features have similar scales. This is particularly important because K-means relies on distance measures, and differences in feature scales can bias the clustering.<br>\n",
    "Common methods include z-score standardization (subtracting the mean and dividing by the standard deviation) or min-max scaling (scaling features to a specific range, e.g., [0, 1] or [-1, 1]).<br>\n",
    "\n",
    "#### 4. Outlier Detection and Handling:\n",
    "\n",
    "Identify and address outliers in your dataset, as they can significantly affect K-means clustering results. You can remove, transform, or down-weight outliers based on the nature of your data and domain knowledge.<br>\n",
    "Sometimes it is difficult to decide on if the points should be treated as outliers. Some rules to follow:\n",
    "- if it is less than 1% of cases -- deletion. could be a good option\n",
    "- if it is ~10% -- it is more likely some type of behavior OR a fraud. If in doubt, try to consult with client on strange behavior, they may have a clue on if it is a standard one.\n",
    "\n",
    "#### 5. Feature Engineering:\n",
    "\n",
    "Consider creating new features or transformations of existing features that may enhance the clustering process. Feature engineering can help capture underlying patterns in the data.<br>\n",
    "Check any collinearity between the variables. If there is any, consider removing variables or transforming them.<br>\n",
    "Try to understand the meaning of the variable to prepare new ones. For example, it could be usefult to make a variable `add_to_cart_per_view` from total `add_to_cart` and total `view_item` events.\n",
    "\n",
    "#### 6. Dimensionality Reduction:\n",
    "\n",
    "If you have a high-dimensional dataset, consider applying dimensionality reduction techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) to reduce the number of features while retaining the most important information.<br>\n",
    "Another good embedding tool is LightFM library working on the simple Neural Network (you can find example in this Notebook).<br>\n",
    "Embeddings also could be goot tool to search for underlying patterns and they will eliminate multicollinearity.<br>\n",
    "\n",
    "#### 7. Encoding Categorical Variables:\n",
    "\n",
    "If your dataset contains categorical variables, you need to encode them into numerical values. Common methods include one-hot encoding or label encoding, depending on the nature of the categorical data.\n",
    "\n",
    "### Understanding K-means output\n",
    "After the data is prepared, we can run K-means.<br>\n",
    "However, to do so, we need to choose amount of clusters to run.<br>\n",
    "To do that several steps are required.\n",
    "\n",
    "### THE NUMBERS\n",
    "\n",
    "#### 1. Selection of Distance Metric:\n",
    "\n",
    "Choose an appropriate distance metric based on the nature of your data. Euclidean distance is the default and widely used, but other metrics like Manhattan distance, cosine similarity, or custom distance functions may be more suitable for specific data types.\n",
    "\n",
    "#### 2.Determining the Number of Clusters (K):\n",
    "\n",
    "Decide on the number of clusters you want to identify. You can use methods like the elbow method, silhouette score, or domain knowledge to help determine the optimal K value.\n",
    "Initializations:\n",
    "\n",
    "#### 3. Choose an initialization method for the cluster centroids.\n",
    "K-means++ is a common choice as it tends to lead to better convergence. You can also experiment with random initialization.\n",
    "Convergence Criteria:\n",
    "\n",
    "#### 4. Set a stopping criterion to determine when the algorithm has converged.\n",
    "Common criteria include a maximum number of iterations or when the centroids no longer change significantly.\n",
    "Data Visualization:\n",
    "\n",
    "#### 5. Metrics.\n",
    "Use metrics such as `calinski_harabasz score` or `davies_bouldin score` to decide on what amount of clusters would work the best.<br>\n",
    "The higher calinski_harabasz the better, the lower davies_bouldin the better.\n",
    "\n",
    "### THE VISUALIZATION\n",
    "\n",
    "#### 1. Construct the plot\n",
    "Plotting will give you a sence on if the chosen amount of data is actually meaningful in the dimentions chosen.\n",
    "Statistics are calculated on distances and can not 100% determine the best clusters number option, so you have to see the possible solutions on the plot. Compare several outputs.\n",
    "\n",
    "### THE INTERPRETATION\n",
    "\n",
    "#### 1. Use clusters to see the data\n",
    "Add clusters to the data and run research to see the difference between the clusters. Do they differ from each other? Could it be explained? Is there meaning behind the clusters? Do you consider the need in other variables to explain it better?\n",
    "The interpretation is the final part where you decide if the chosen clusters are good enough and if you need more data to re-run the algorithm.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "Before running K-means, consider visualizing your data using scatter plots, PCA plots, or other techniques to gain insights into the data structure.\n",
    "Data Splitting:\n",
    "\n",
    "Consider splitting your data into a training set for clustering and a separate test set for evaluating the results. This helps avoid overfitting and ensures that your clustering model generalizes well to new data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
